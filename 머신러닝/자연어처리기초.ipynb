{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLypSJKaP7REWIe+7MuL2M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["NLTK 설치 및 기본 데이터 다운로드"],"metadata":{"id":"UisRLU5cHbMM"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MouiB0CpErRe","executionInfo":{"status":"ok","timestamp":1745889022462,"user_tz":-540,"elapsed":353,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"c2c57a3c-3c63-44df-b018-a812f76e18c1"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["# NLTK 설치\n","# pip install nltk\n","\n","# 필요한 데이터 다운로드\n","import nltk\n","nltk.download('punkt')    # 토큰화 모듈\n","nltk.download('stopwords')  # 불용어 데이터\n","nltk.download('punkt_tab')"]},{"cell_type":"markdown","source":["기본 토큰화 실습"],"metadata":{"id":"edeJWnHEHZfW"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","# 예제 텍스트\n","text = \"Natural Language Processing is exciting and fun! It has many applications in real world.\"\n","\n","# 문장 토큰화\n","sentences = sent_tokenize(text)\n","print(\"문장 토큰화 결과:\")\n","for i, sentence in enumerate(sentences):\n","    print(f\"문장 {i+1}: {sentence}\")\n","\n","# 단어 토큰화\n","tokens = word_tokenize(text)\n","print(\"\\\\n단어 토큰화 결과:\")\n","print(tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYknZbjsFaSd","executionInfo":{"status":"ok","timestamp":1745889023619,"user_tz":-540,"elapsed":131,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"b0b3ebc2-8ac1-4c85-97bf-7faa8074a361"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["문장 토큰화 결과:\n","문장 1: Natural Language Processing is exciting and fun!\n","문장 2: It has many applications in real world.\n","\\n단어 토큰화 결과:\n","['Natural', 'Language', 'Processing', 'is', 'exciting', 'and', 'fun', '!', 'It', 'has', 'many', 'applications', 'in', 'real', 'world', '.']\n"]}]},{"cell_type":"markdown","source":["불용어(Stopwords) 제거 실습"],"metadata":{"id":"v4CTfzPaHVfx"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","\n","# 불용어 목록 가져오기\n","stop_words = set(stopwords.words('english'))\n","\n","# 불용어 제거\n","filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n","\n","print(\"\\\\n원본 토큰:\", tokens)\n","print(\"불용어 제거 후:\", filtered_tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qRJjfLGzFyiu","executionInfo":{"status":"ok","timestamp":1745889349241,"user_tz":-540,"elapsed":10,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"3cbde731-a52f-4a8e-be11-c055823f9448"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\\n원본 토큰: ['Natural', 'Language', 'Processing', 'is', 'exciting', 'and', 'fun', '!', 'It', 'has', 'many', 'applications', 'in', 'real', 'world', '.']\n","불용어 제거 후: ['Natural', 'Language', 'Processing', 'exciting', 'fun', '!', 'many', 'applications', 'real', 'world', '.']\n"]}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","\n","# 불용어 목록 가져오기\n","stop_words_1 = set(stopwords.words('english'))\n","stop_words_2 = {'!', '.'}\n","stop_words = stop_words_1.union(stop_words_2)\n","# 불용어 제거\n","filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n","\n","print(\"\\\\n원본 토큰:\", tokens)\n","print(\"불용어 제거 후:\", filtered_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpUhp5tKHU2A","executionInfo":{"status":"ok","timestamp":1745890476004,"user_tz":-540,"elapsed":53,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"2cc8bae0-4e49-4a48-9f23-cf12c8427140"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\\n원본 토큰: ['Natural', 'Language', 'Processing', 'is', 'exciting', 'and', 'fun', '!', 'It', 'has', 'many', 'applications', 'in', 'real', 'world', '.']\n","불용어 제거 후: ['Natural', 'Language', 'Processing', 'is', 'exciting', 'and', 'fun', 'It', 'has', 'many', 'applications', 'in', 'real', 'world']\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","# 예제 텍스트\n","text = \"자연어 처리는 흥미롭고 재미있어요! 현실 세계에는 많은 응용 프로그램이 있습니다.\"\n","\n","# 문장 토큰화\n","sentences = sent_tokenize(text)\n","print(\"문장 토큰화 결과:\")\n","for i, sentence in enumerate(sentences):\n","    print(f\"문장 {i+1}: {sentence}\")\n","\n","# 단어 토큰화\n","tokens = word_tokenize(text)\n","print(\"\\\\n단어 토큰화 결과:\")\n","print(tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMmRLwD7Ln63","executionInfo":{"status":"ok","timestamp":1745890997995,"user_tz":-540,"elapsed":58,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"778353e3-f4ae-4ea3-89bf-40f429066539"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["문장 토큰화 결과:\n","문장 1: 자연어 처리는 흥미롭고 재미있어요!\n","문장 2: 현실 세계에는 많은 응용 프로그램이 있습니다.\n","\\n단어 토큰화 결과:\n","['자연어', '처리는', '흥미롭고', '재미있어요', '!', '현실', '세계에는', '많은', '응용', '프로그램이', '있습니다', '.']\n"]}]},{"cell_type":"code","source":["# 필요한 패키지 설치\n","!pip install konlpy\n","\n","from konlpy.tag import Okt\n","\n","# 한국어 불용어 정의\n","korean_stopwords = {\n","    '이', '그', '저', '것', '이것', '저것', '그것',\n","    '나', '너', '우리', '저희', '당신', '그들', '그녀', '그분',\n","    '이런', '그런', '저런', '어떤', '무슨', '어느',\n","    '은', '는', '이', '가', '을', '를', '에', '의', '로', '으로',\n","    '과', '와', '이나', '나', '또는', '혹은',\n","    '하다', '있다', '되다', '같다',\n","    '및', '또한', '그리고', '따라서', '그러나', '하지만',\n","    '매우', '아주', '너무', '참', '정말', '그냥',\n","    '때문에', '그래서', '그러므로', '그러면', '그러니까',\n","    '이제', '곧', '즉', '곧바로', '바로',\n","    '예', '아니', '네', '그렇다', '아니다'\n","}\n","\n","# 예제 텍스트\n","text = \"자연어 처리는 매우 흥미롭고 재미있습니다! 실제 세계에서 많은 응용이 가능합니다.\"\n","\n","# Okt 형태소 분석기 초기화\n","okt = Okt()\n","\n","# 형태소 분석\n","morphs = okt.morphs(text)  # 형태소 단위로 분리\n","print(\"\\n원본 형태소:\", morphs)\n","\n","# 불용어 제거\n","filtered_morphs = [word for word in morphs if word not in korean_stopwords]\n","print(\"불용어 제거 후 형태소:\", filtered_morphs)\n","\n","# 품사 태깅 결과에서 불용어 제거\n","pos_tagged = okt.pos(text)\n","print(\"\\n품사 태깅 결과:\")\n","for word, tag in pos_tagged:\n","    print(f\"{word}/{tag}\")\n","\n","# 명사, 동사, 형용사만 추출 (품사 기반 불용어 제거)\n","content_words = []\n","for word, tag in pos_tagged:\n","    # 명사(Noun), 동사(Verb), 형용사(Adjective)만 선택\n","    if tag in ['Noun', 'Verb', 'Adjective']:\n","        content_words.append(word)\n","\n","print(\"\\n주요 품사만 추출 (명사, 동사, 형용사):\")\n","print(content_words)\n","\n","# 명사만 추출\n","nouns = okt.nouns(text)\n","print(\"\\n명사만 추출:\")\n","print(nouns)\n","\n","# 통계\n","print(\"\\n통계:\")\n","print(f\"전체 형태소 수: {len(morphs)}\")\n","print(f\"불용어 제거 후 형태소 수: {len(filtered_morphs)}\")\n","print(f\"주요 품사 단어 수: {len(content_words)}\")\n","print(f\"명사 수: {len(nouns)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSYgosq4Nprj","executionInfo":{"status":"ok","timestamp":1745891361777,"user_tz":-540,"elapsed":14472,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"d4d3270f-bcaa-4ea7-95ef-25cfc8c7c0f4"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.2 konlpy-0.6.0\n","\n","원본 형태소: ['자연어', '처리', '는', '매우', '흥미', '롭고', '재미있습니다', '!', '실제', '세계', '에서', '많은', '응용', '이', '가능합니다', '.']\n","불용어 제거 후 형태소: ['자연어', '처리', '흥미', '롭고', '재미있습니다', '!', '실제', '세계', '에서', '많은', '응용', '가능합니다', '.']\n","\n","품사 태깅 결과:\n","자연어/Noun\n","처리/Noun\n","는/Josa\n","매우/Noun\n","흥미/Noun\n","롭고/Josa\n","재미있습니다/Adjective\n","!/Punctuation\n","실제/Noun\n","세계/Noun\n","에서/Josa\n","많은/Adjective\n","응용/Noun\n","이/Josa\n","가능합니다/Adjective\n","./Punctuation\n","\n","주요 품사만 추출 (명사, 동사, 형용사):\n","['자연어', '처리', '매우', '흥미', '재미있습니다', '실제', '세계', '많은', '응용', '가능합니다']\n","\n","명사만 추출:\n","['자연어', '처리', '매우', '흥미', '실제', '세계', '응용']\n","\n","통계:\n","전체 형태소 수: 16\n","불용어 제거 후 형태소 수: 13\n","주요 품사 단어 수: 10\n","명사 수: 7\n"]}]},{"cell_type":"markdown","source":["어간 추출(Stemming)과 표제어 추출(Lemmatization) 실습"],"metadata":{"id":"m7BZsKyAQHaF"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","\n","# WordNet 데이터 다운로드\n","nltk.download('wordnet')\n","\n","# 어간 추출기와 표제어 추출기 초기화\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# 어간 추출 예제\n","words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n","stemmed_words = [stemmer.stem(word) for word in words]\n","\n","print(\"\\\\n어간 추출 결과:\")\n","for original, stemmed in zip(words, stemmed_words):\n","    print(f\"{original} -> {stemmed}\")\n","\n","# 표제어 추출 예제\n","words = [\"running\", \"runs\", \"ran\", \"better\", \"goods\"]\n","lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","\n","print(\"\\\\n표제어 추출 결과:\")\n","for original, lemmatized in zip(words, lemmatized_words):\n","    print(f\"{original} -> {lemmatized}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWsQRQXgQJGY","executionInfo":{"status":"ok","timestamp":1745891864095,"user_tz":-540,"elapsed":4728,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"4c67224f-3203-4766-a58d-b32b508c4c4b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["\\n어간 추출 결과:\n","running -> run\n","runs -> run\n","ran -> ran\n","easily -> easili\n","fairly -> fairli\n","\\n표제어 추출 결과:\n","running -> running\n","runs -> run\n","ran -> ran\n","better -> better\n","goods -> good\n"]}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","\n","# WordNet 데이터 다운로드\n","nltk.download('wordnet')\n","\n","# 어간 추출기와 표제어 추출기 초기화\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# 어간 추출 예제\n","words = [\"달리다\", \"달렸다\", \"달리냐\", \"쉬운\", \"공정한\"]\n","stemmed_words = [stemmer.stem(word) for word in words]\n","\n","print(\"\\\\n어간 추출 결과:\")\n","for original, stemmed in zip(words, stemmed_words):\n","    print(f\"{original} -> {stemmed}\")\n","\n","# 표제어 추출 예제\n","words = [\"달리다\", \"달렸다\", \"달리냐\", \"쉬운\", \"공정한\"]\n","lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","\n","print(\"\\\\n표제어 추출 결과:\")\n","for original, lemmatized in zip(words, lemmatized_words):\n","    print(f\"{original} -> {lemmatized}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAIUns1VQ7p4","executionInfo":{"status":"ok","timestamp":1745891922450,"user_tz":-540,"elapsed":35,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"3d1bd875-5df9-4344-b9a9-cf5b40f3d782"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\\n어간 추출 결과:\n","달리다 -> 달리다\n","달렸다 -> 달렸다\n","달리냐 -> 달리냐\n","쉬운 -> 쉬운\n","공정한 -> 공정한\n","\\n표제어 추출 결과:\n","달리다 -> 달리다\n","달렸다 -> 달렸다\n","달리냐 -> 달리냐\n","쉬운 -> 쉬운\n","공정한 -> 공정한\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["# 필요한 패키지 설치\n","# pip install konlpy\n","\n","from konlpy.tag import Okt  # KoNLPy의 Okt 형태소 분석기 사용\n","\n","# 예제 한국어 텍스트\n","text = \"자연어 처리는 매우 흥미롭고 재미있습니다! 실제 세계에서 많은 응용이 가능합니다.\"\n","\n","# Okt 형태소 분석기 초기화\n","okt = Okt()\n","\n","# 문장 분리 (한국어 문장 분리를 위해 간단한 규칙 적용)\n","sentences = [s.strip() for s in text.split('!') if s.strip()]\n","print(\"문장 분리 결과:\")\n","for i, sentence in enumerate(sentences):\n","    print(f\"문장 {i+1}: {sentence}\")\n","\n","# 형태소 분석\n","morphs = okt.morphs(text)  # 형태소 단위로 분리\n","pos = okt.pos(text)        # 품사 태깅\n","\n","print(\"\\n형태소 분석 결과:\")\n","print(morphs)\n","\n","print(\"\\n품사 태깅 결과:\")\n","for word, tag in pos:\n","    print(f\"{word}/{tag}\")\n","\n","# 어간 추출 (동사/형용사의 어간 추출)\n","print(\"\\n어간 추출 결과:\")\n","for word, tag in pos:\n","    if tag in ['Verb', 'Adjective']:  # 동사나 형용사인 경우\n","        # '다'로 끝나는 경우 어간으로 간주\n","        if word.endswith('다'):\n","            print(f\"{word} -> {word[:-1]}\")\n","        # '고'로 끝나는 경우\n","        elif word.endswith('고'):\n","            print(f\"{word} -> {word[:-1]}다\")\n","        else:\n","            print(f\"{word} -> {word}\")\n","\n","# 표제어 추출\n","print(\"\\n표제어 추출 결과:\")\n","for word, tag in pos:\n","    if tag in ['Verb', 'Adjective']:  # 동사나 형용사인 경우\n","        # '다'로 끝나는 경우\n","        if word.endswith('다'):\n","            print(f\"{word} -> {word}\")\n","        # '고'로 끝나는 경우\n","        elif word.endswith('고'):\n","            print(f\"{word} -> {word[:-1]}다\")\n","        # '습니다'로 끝나는 경우\n","        elif word.endswith('습니다'):\n","            stem = word[:-3]  # '습니다' 제거\n","            if stem.endswith('있'):  # '있습니다'의 경우\n","                print(f\"{word} -> 있다\")\n","            elif stem.endswith('었'):  # '었습니다'의 경우\n","                print(f\"{word} -> {stem[:-1]}다\")\n","            else:\n","                print(f\"{word} -> {stem}다\")\n","        # '어요'로 끝나는 경우\n","        elif word.endswith('어요'):\n","            print(f\"{word} -> {word[:-2]}다\")\n","        else:\n","            print(f\"{word} -> {word}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qpHI6YSRbYw","executionInfo":{"status":"ok","timestamp":1745892000058,"user_tz":-540,"elapsed":85,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"5fd0edfb-7507-411a-8f96-410e0419fff2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["문장 분리 결과:\n","문장 1: 자연어 처리는 매우 흥미롭고 재미있습니다\n","문장 2: 실제 세계에서 많은 응용이 가능합니다.\n","\n","형태소 분석 결과:\n","['자연어', '처리', '는', '매우', '흥미', '롭고', '재미있습니다', '!', '실제', '세계', '에서', '많은', '응용', '이', '가능합니다', '.']\n","\n","품사 태깅 결과:\n","자연어/Noun\n","처리/Noun\n","는/Josa\n","매우/Noun\n","흥미/Noun\n","롭고/Josa\n","재미있습니다/Adjective\n","!/Punctuation\n","실제/Noun\n","세계/Noun\n","에서/Josa\n","많은/Adjective\n","응용/Noun\n","이/Josa\n","가능합니다/Adjective\n","./Punctuation\n","\n","어간 추출 결과:\n","재미있습니다 -> 재미있습니\n","많은 -> 많은\n","가능합니다 -> 가능합니\n","\n","표제어 추출 결과:\n","재미있습니다 -> 재미있습니다\n","많은 -> 많은\n","가능합니다 -> 가능합니다\n"]}]},{"cell_type":"markdown","source":["Bag of Words 구현 실습"],"metadata":{"id":"9m6BZJwWSddG"}},{"cell_type":"code","source":["from collections import Counter\n","\n","# 예제 문서\n","documents = [\n","    \"Natural language processing is fascinating\",\n","    \"I love learning about NLP and its applications\",\n","    \"Machine learning and NLP go hand in hand\"\n","]\n","\n","# 각 문서의 단어 빈도 계산\n","def create_bow(document):\n","    # 소문자 변환 및 토큰화\n","    tokens = word_tokenize(document.lower())\n","    # 불용어 제거\n","    filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token.isalpha()]\n","    # 단어 빈도 계산\n","    word_freq = Counter(filtered_tokens)\n","    return word_freq\n","\n","# 각 문서의 BOW 생성\n","bow_results = [create_bow(doc) for doc in documents]\n","\n","# 결과 출력\n","print(\"\\\\nBag of Words 결과:\")\n","for i, bow in enumerate(bow_results):\n","    print(f\"\\\\n문서 {i+1}:\")\n","    for word, count in bow.items():\n","        print(f\"  {word}: {count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvdTZpb7Sds5","executionInfo":{"status":"ok","timestamp":1745892275955,"user_tz":-540,"elapsed":53,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"90459911-fc80-46ce-f69f-7c646c8dc1d1"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\\nBag of Words 결과:\n","\\n문서 1:\n","  natural: 1\n","  language: 1\n","  processing: 1\n","  is: 1\n","  fascinating: 1\n","\\n문서 2:\n","  i: 1\n","  love: 1\n","  learning: 1\n","  about: 1\n","  nlp: 1\n","  and: 1\n","  its: 1\n","  applications: 1\n","\\n문서 3:\n","  machine: 1\n","  learning: 1\n","  and: 1\n","  nlp: 1\n","  go: 1\n","  hand: 2\n","  in: 1\n"]}]},{"cell_type":"markdown","source":["TF-IDF 구현 실습"],"metadata":{"id":"PeuMA6HxUaHz"}},{"cell_type":"code","source":["import math\n","from collections import Counter, defaultdict\n","\n","# 예제 문서\n","documents = [\n","    \"Natural language processing is fascinating\",\n","    \"I love learning about NLP and its applications\",\n","    \"Machine learning and NLP go hand in hand\"\n","]\n","\n","# 각 문서 토큰화\n","tokenized_docs = []\n","for doc in documents:\n","    tokens = word_tokenize(doc.lower())\n","    filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token.isalpha()]\n","    tokenized_docs.append(filtered_tokens)\n","\n","# 전체 어휘 사전 생성\n","vocab = set()\n","for doc in tokenized_docs:\n","    vocab.update(doc)\n","\n","# 단어별 문서 빈도(DF) 계산\n","doc_freq = defaultdict(int)\n","for doc in tokenized_docs:\n","    for word in set(doc):  # 각 문서에서 한 번만 카운트\n","        doc_freq[word] += 1\n","\n","# TF-IDF 계산 함수\n","def compute_tfidf(doc, doc_index):\n","    N = len(documents)  # 전체 문서 수\n","    tfidf_scores = {}\n","\n","    # 단어 빈도(TF) 계산\n","    word_freq = Counter(doc)\n","    doc_len = len(doc)\n","\n","    for word, freq in word_freq.items():\n","        # TF 계산 (정규화된 빈도)\n","        tf = freq / doc_len\n","        # IDF 계산\n","        idf = math.log(N / doc_freq[word])\n","        # TF-IDF 계산\n","        tfidf_scores[word] = tf * idf\n","\n","    return tfidf_scores\n","\n","# 각 문서의 TF-IDF 계산\n","tfidf_results = []\n","for i, doc in enumerate(tokenized_docs):\n","    tfidf_scores = compute_tfidf(doc, i)\n","    tfidf_results.append(tfidf_scores)\n","\n","# 결과 출력\n","print(\"\\\\nTF-IDF 결과:\")\n","for i, tfidf in enumerate(tfidf_results):\n","    print(f\"\\\\n문서 {i+1}:\")\n","    # 점수 기준 내림차순 정렬\n","    sorted_tfidf = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n","    for word, score in sorted_tfidf:\n","        print(f\"  {word}: {score:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-zePWMIfUaa_","executionInfo":{"status":"ok","timestamp":1745892787062,"user_tz":-540,"elapsed":55,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"89583264-99da-4594-9705-64acc452ac04"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\\nTF-IDF 결과:\n","\\n문서 1:\n","  natural: 0.2197\n","  language: 0.2197\n","  processing: 0.2197\n","  is: 0.2197\n","  fascinating: 0.2197\n","\\n문서 2:\n","  i: 0.1373\n","  love: 0.1373\n","  about: 0.1373\n","  its: 0.1373\n","  applications: 0.1373\n","  learning: 0.0507\n","  nlp: 0.0507\n","  and: 0.0507\n","\\n문서 3:\n","  hand: 0.2747\n","  machine: 0.1373\n","  go: 0.1373\n","  in: 0.1373\n","  learning: 0.0507\n","  and: 0.0507\n","  nlp: 0.0507\n"]}]},{"cell_type":"markdown","source":["NLTK 기본 토큰화와 불용어 제거"],"metadata":{"id":"POChbx_gVwkU"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","# 예제 텍스트\n","text = \"Natural Language Processing is exciting and fun!\"\n","\n","# 토큰화\n","tokens = word_tokenize(text)\n","\n","# 영어 불용어 세트 가져오기\n","stop_words = set(stopwords.words('english'))\n","\n","# 불용어 제거\n","filtered = [t for t in tokens if t.lower() not in stop_words]\n","\n","# 결과 출력\n","print('원본:', tokens)\n","print('불용어 제거:', filtered)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vGF4c_4dVw12","executionInfo":{"status":"ok","timestamp":1745893138327,"user_tz":-540,"elapsed":118,"user":{"displayName":"정승원","userId":"12870979077742307239"}},"outputId":"cdba6f28-4670-44dc-e182-e51157c0cc7e"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["원본: ['Natural', 'Language', 'Processing', 'is', 'exciting', 'and', 'fun', '!']\n","불용어 제거: ['Natural', 'Language', 'Processing', 'exciting', 'fun', '!']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-eKg5c7IVx40"},"execution_count":null,"outputs":[]}]}