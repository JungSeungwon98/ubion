{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### model 임포트\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K3t-1RUwYvZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rZ54bsnX9G6"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_google_genai langchain_groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0J1g9d5rkOx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"]=\"\"\n",
        "os.environ[\"TAVILY_API_KEY\"]=\"tvly-dev-\"\n",
        "os.environ[\"GOOGLE_API_KEY\"]=\"\""
      ],
      "metadata": {
        "id": "TPKQJaJYZNuh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\"\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\"\n",
        ")\n",
        "llm_groq = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0,\n",
        "    max_retries=2\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "oG1dxlYkdLNG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm.invoke(\"langgraph에 관한 발라드 음악을 작사해주세요\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "yBhlNlSgeuSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = llm_groq.invoke(\"langgraph에 관한 발라드 음악을 작사해주세요\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "DflQ-5nrii2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever 만들기"
      ],
      "metadata": {
        "id": "bSNrrHSkjC3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]"
      ],
      "metadata": {
        "id": "a1kvG4BIi0iX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "e2S5d9nHjzYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "BQZCtnBlkDaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "#3개의 사이트에서 가져온 문서를 chunck 단위로 나눈것을 list\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list =[item for sublist in docs for item in sublist]\n",
        "# docs_list\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=10\n",
        ")\n",
        "\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"langgraph\",\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "VDkqbOXUjPaz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain RAG"
      ],
      "metadata": {
        "id": "pBgKka57oeaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "#promt\n",
        "promt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "#Post preprocessing\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "#rag chain\n",
        "rag_chain = promt | llm | StrOutputParser()\n",
        "rag_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_egpDoatod6A",
        "outputId": "ffba6e71-4400-4f87-b1cf-b27da554d5fa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])\n",
              "| ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7848fcc50a50>, default_metadata=(), model_kwargs={})\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG TEST"
      ],
      "metadata": {
        "id": "DFaB2vHzqF2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#run\n",
        "question = \"tell me about agent memory\"\n",
        "generation = rag_chain.invoke({\"context\":docs, \"question\":question})\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOAYxepPk8vG",
        "outputId": "c9efef9a-6bdd-40b0-f724-d798823d56fa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent memory is a key component in LLM-powered autonomous agent systems, providing the ability to retain and recall information. It includes short-term memory, utilizing in-context learning, and long-term memory, leveraging an external vector store for extended information retention. This memory enables agents to learn from past actions and improve future results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### grade document class"
      ],
      "metadata": {
        "id": "0_l3KMArr5ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class GradeDocuments(BaseModel):\n",
        "  \"\"\"\n",
        "  Binary score for relevance check on retrieved documents.\n",
        "  \"\"\"\n",
        "  binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n"
      ],
      "metadata": {
        "id": "IMb3JBhFrtGx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm_grader =llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "\n",
        "ko_system = \"\"\"\n",
        "    검색된 문서와 사용자 질문의 관련성을 평가하는 채점자입니다.\\n\n",
        "    문서에 질문과 관련된 키워드 또는 의미론적 의미가 포함된 경우 관련성이 있는 것으로 평가합니다.\\n\n",
        "    문서가 질문과 관련이 있는지 여부를 나타내기 위해 이진 점수 '예' 또는 '아니오'를 제공합니다.\n",
        "\"\"\"\n",
        "\n",
        "grade_promt =ChatPromptTemplate.from_messages(\n",
        "    {\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    }\n",
        ")\n",
        "\n",
        "retriever_grader = grade_promt | structured_llm_grader"
      ],
      "metadata": {
        "id": "91rVu5ZBuB2d"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"tell me about agent memory\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "docs_txt = docs[0].page_content\n",
        "print(retriever_grader.invoke({\"question\":question , \"document\":docs_txt}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p1O2BtvwEet",
        "outputId": "726e6372-6a49-436d-b3a9-4edeb3efa7c8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"tell me about agent seoul\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "docs_txt = docs[0].page_content\n",
        "print(retriever_grader.invoke({\"question\":question , \"document\":docs_txt}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VjjjqE7wZMS",
        "outputId": "64702f18-a556-4fbc-d30d-ea007a18ad60"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binary_score='no'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### question Rewriter"
      ],
      "metadata": {
        "id": "xzTp6vhdx37u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Question Re-writer\n",
        "# Prompt\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "\n",
        "\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "pR0nBm9dxibS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KEaB3Z6FyP6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_rewriter.invoke({\"question\":question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "OSSR93SayIYX",
        "outputId": "902168f3-6bb8-44d5-da01-ce4c8abaf9dc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here are a few options for improved search queries, depending on the specific intent:\\n\\n*   **Option 1 (General Information):** \"Who is Agent Seoul?\" (This is a direct and simple rephrase)\\n*   **Option 2 (If \"Agent Seoul\" is likely related to a specific organization or media):** \"Agent Seoul [Organization Name/Movie/Game]\" (e.g., \"Agent Seoul Valorant\", \"Agent Seoul Netflix\")\\n*   **Option 3 (If looking for real-world intelligence agents):** \"Seoul intelligence agents\" or \"South Korean intelligence agencies\"\\n*   **Option 4 (If context suggests a fictional character):** \"Agent Seoul character description\" or \"Agent Seoul fictional character\"\\n\\nThe best option depends on what \"Agent Seoul\" refers to. Without more context, the first option is the safest.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Search\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ],
      "metadata": {
        "id": "2sAkZYkWyRe8"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "web_search_tool({\"query\":\"tell me about Taj Mahal\"})[1]"
      ],
      "metadata": {
        "id": "ptvDS8Fqyt1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWoQ5UEWy1mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### langgrap 실습 및 구현"
      ],
      "metadata": {
        "id": "QPJ7Ku6UHrPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### state 정의"
      ],
      "metadata": {
        "id": "3Szlv-H3IFRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List , Dict , Any ,TypedDict ,Annotated\n",
        "from langchain.schema import Document\n",
        "\n",
        "#전달해주는 state 상태정의\n",
        "class State(TypedDict):\n",
        "  question:str\n",
        "  orginal_question:str\n",
        "  documents:List[Document]\n",
        "  web_search:str\n",
        "  generation:str\n",
        "  web_results:List[Dict[str,Any]]\n",
        "  relevance_score:str\n",
        "\n",
        "class GraphState(State):\n",
        "  question:Annotated[str , \"user qeustion\" ]\n",
        "  documents:Annotated[List[Document] , []]\n",
        "  orginal_question:Annotated[str , \"original question\" ]\n",
        "  web_search:Annotated[str , \"web search\" ]\n",
        "  generation:Annotated[str , \"generation\" ]\n",
        "  web_results:Annotated[List[Dict[str,Any]] , [{}]]\n",
        "  relevance_score:Annotated[str , \"relevance_score\" ]"
      ],
      "metadata": {
        "id": "9rGfxGFsHux9"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Node 정의\n"
      ],
      "metadata": {
        "id": "hwjwMkavKvgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieve 정의함수\n",
        "def retrieve(state:GraphState):\n",
        "  print(\"---Retrieve---\")\n",
        "  question = state[\"question\"]\n",
        "\n",
        "  #검색 실행\n",
        "  documents = retriever.get_relevant_documents(question)\n",
        "  # return State(documents=[f\"jfwpjfpewpfwejpfjpwef{question}\"])\n",
        "  return GraphState(question=question, documents=documents)"
      ],
      "metadata": {
        "id": "UV4QZ1rjIQJs"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = retrieve(State(question=\"tell me about SEOUL\"))\n",
        "# documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWWUlQAyMFFy",
        "outputId": "59c85639-b990-40da-fe1f-84711d7129a9"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Retrieve---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#grade 노드\n",
        "def grade_documents(state:GraphState):\n",
        "  \"\"\"\n",
        "  check document relevance to question\n",
        "  \"\"\"\n",
        "  print(\"---CHECKING DOCUMENT RELEVANT IS TO QUESTION OR NOT---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  #문서가 없으면 바로 웹 검색하도록\n",
        "  if not documents:\n",
        "    return GraphState(\n",
        "        question=question,\n",
        "        orginal_question=state.get(\"original_questrion\", question),\n",
        "        documents=[],\n",
        "        web_search=\"yes\",\n",
        "        web_results=[{}],\n",
        "        relevance_score=\"no\")\n",
        "\n",
        "  # 문서 평가\n",
        "  filtered_docs = []\n",
        "  web_search =\"no\"\n",
        "  relevant_count = 0\n",
        "  for document in documents:\n",
        "    score= retriever_grader.invoke(\n",
        "        {\n",
        "            \"question\":question,\n",
        "            \"document\":document.page_content\n",
        "        }\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "    if grade == \"yes\":\n",
        "      print(\"---GRADE : DOCUMENT RELEVANT---\")\n",
        "      filtered_docs.append(document)\n",
        "      relevant_count +=1\n",
        "    else:\n",
        "      print(\"---GRADE : DOCUMENT NOT RELEVANT---\")\n",
        "\n",
        "  #관련성 너무 적으면 웹서치 할 수 있도록 함\n",
        "  if relevant_count < 2:\n",
        "    web_search = \"yes\"\n",
        "    print(f\"---ONLY {relevant_count} RELEVANT DOCUMENTS, WEB SEARCH NEEDED---\")\n",
        "  # return GraphState(\n",
        "  #     question=question,\n",
        "  #     orginal_question=state.get(\"original_questrion\", question),\n",
        "  #     documents=filtered_docs,\n",
        "  #     web_search=web_search,\n",
        "  #     web_results=[{}],\n",
        "  #     relevance_score=\"yes\" if filtered_docs else \"no\"\n",
        "  # )\n",
        "\n",
        "  return GraphState(question=question,web_search=web_search,documents=filtered_docs,)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RvKFKHGxMHrw"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grade_state = grade_documents(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeFD5lyZS6Fs",
        "outputId": "fa2c4ea9-36fc-4158-8446-c29106cabd82"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---CHECKING DOCUMENT RELEVANT IS TO QUESTION OR NOT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---ONLY 0 RELEVANT DOCUMENTS, WEB SEARCH NEEDED---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Question Re-writer\n",
        "# Prompt\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "\n",
        "\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "tsK5Cm8LYvIr"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_query(state:GraphState):\n",
        "  \"\"\"질문 재작성\"\"\"\n",
        "  print(\"---TRANSFORM QUERY---\")\n",
        "\n",
        "  #처음 질문 저장\n",
        "  original_question = state.get(\"original_question\", state[\"question\"])\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "  #질문 재작성\n",
        "  better_question = question_rewriter.invoke({\"question\":question})\n",
        "  return GraphState(\n",
        "      question=better_question ,\n",
        "      orginal_question=original_question,\n",
        "      documents=documents\n",
        "      )"
      ],
      "metadata": {
        "id": "OzI8U1C_TUFd"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_query(grade_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrySAVo1ZGNn",
        "outputId": "a6f6282e-8ca0-4599-84d2-10a4dcae2da5"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---TRANSFORM QUERY---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Here are a few options for re-writing the question, depending on the specific intent:\\n\\n**Option 1 (General Overview):**\\n\\n*   **Search Query:** \"Seoul, South Korea: history, culture, and attractions\"\\n\\n**Option 2 (Focus on Travel/Tourism):**\\n\\n*   **Search Query:** \"Top tourist attractions and things to do in Seoul\"\\n\\n**Option 3 (Focus on Current Events/News):**\\n\\n*   **Search Query:** \"Recent news and events in Seoul, South Korea\"\\n\\n**Option 4 (Focus on History):**\\n\\n*   **Search Query:** \"History of Seoul, South Korea: from ancient times to present\"\\n\\n**Explanation of Improvements:**\\n\\n*   **Specificity:** The original question \"tell me about SEOUL\" is very broad. The re-written options add context (e.g., \"South Korea\", \"history\", \"tourist attractions\") to narrow the search and provide more relevant results.\\n*   **Keywords:** Using keywords like \"tourist attractions,\" \"history,\" and \"news\" helps search engines understand the desired information.\\n*   **Phrasing:** Using common search phrases (e.g., \"things to do in...\") increases the likelihood of finding relevant articles and websites.',\n",
              " 'orginal_question': 'tell me about SEOUL',\n",
              " 'documents': []}"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qDkiUUHeZ78_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}