{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3t-1RUwYvZN"
      },
      "source": [
        "### model 임포트\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rZ54bsnX9G6"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_google_genai langchain_groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J1g9d5rkOx2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPKQJaJYZNuh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"]=\"\"\n",
        "os.environ[\"TAVILY_API_KEY\"]=\"\"\n",
        "os.environ[\"GOOGLE_API_KEY\"]=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "oG1dxlYkdLNG"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\"\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\"\n",
        ")\n",
        "llm_groq = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0,\n",
        "    max_retries=2\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBhlNlSgeuSU"
      },
      "outputs": [],
      "source": [
        "result = llm.invoke(\"langgraph에 관한 발라드 음악을 작사해주세요\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DflQ-5nrii2t"
      },
      "outputs": [],
      "source": [
        "result = llm_groq.invoke(\"langgraph에 관한 발라드 음악을 작사해주세요\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSNrrHSkjC3A"
      },
      "source": [
        "### Retriever 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "a1kvG4BIi0iX"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2S5d9nHjzYa"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQZCtnBlkDaG"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "VDkqbOXUjPaz"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "#3개의 사이트에서 가져온 문서를 chunck 단위로 나눈것을 list\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list =[item for sublist in docs for item in sublist]\n",
        "# docs_list\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=10\n",
        ")\n",
        "\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    embedding=embeddings,\n",
        "    collection_name=\"langgraph\",\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBgKka57oeaf"
      },
      "source": [
        "### LangChain RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_egpDoatod6A",
        "outputId": "61e28c9a-4267-434e-8c94-c46d179f51aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])\n",
              "| ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x786ae820bd50>, default_metadata=(), model_kwargs={})\n",
              "| StrOutputParser()"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "#promt\n",
        "promt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "#Post preprocessing\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "#rag chain\n",
        "rag_chain = promt | llm | StrOutputParser()\n",
        "rag_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFaB2vHzqF2x"
      },
      "source": [
        "### RAG TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOAYxepPk8vG",
        "outputId": "778e1be0-3a34-4326-d5e1-334d9bee8da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent memory in LLMs involves short-term and long-term storage. Short-term memory uses in-context learning, while long-term memory retains information over extended periods using external vector stores for fast retrieval. This external memory helps overcome the limitations of finite attention spans.\n"
          ]
        }
      ],
      "source": [
        "#run\n",
        "question = \"tell me about agent memory\"\n",
        "generation = rag_chain.invoke({\"context\":docs, \"question\":question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_l3KMArr5ka"
      },
      "source": [
        "### grade document class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "IMb3JBhFrtGx"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class GradeDocuments(BaseModel):\n",
        "  \"\"\"\n",
        "  Binary score for relevance check on retrieved documents.\n",
        "  \"\"\"\n",
        "  binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "91rVu5ZBuB2d"
      },
      "outputs": [],
      "source": [
        "structured_llm_grader =llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "\n",
        "ko_system = \"\"\"\n",
        "    검색된 문서와 사용자 질문의 관련성을 평가하는 채점자입니다.\\n\n",
        "    문서에 질문과 관련된 키워드 또는 의미론적 의미가 포함된 경우 관련성이 있는 것으로 평가합니다.\\n\n",
        "    문서가 질문과 관련이 있는지 여부를 나타내기 위해 이진 점수 '예' 또는 '아니오'를 제공합니다.\n",
        "\"\"\"\n",
        "\n",
        "grade_promt =ChatPromptTemplate.from_messages(\n",
        "    {\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    }\n",
        ")\n",
        "\n",
        "retriever_grader = grade_promt | structured_llm_grader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p1O2BtvwEet",
        "outputId": "c8fc0e72-17c3-4de2-b392-e282411ce499"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-56-3ed1023c1372>:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(question)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ],
      "source": [
        "question = \"tell me about agent memory\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "docs_txt = docs[0].page_content\n",
        "print(retriever_grader.invoke({\"question\":question , \"document\":docs_txt}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VjjjqE7wZMS",
        "outputId": "e5de2558-8055-4de0-cef2-11d56b76d0a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binary_score='no'\n"
          ]
        }
      ],
      "source": [
        "question = \"tell me about agent seoul\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "docs_txt = docs[0].page_content\n",
        "print(retriever_grader.invoke({\"question\":question , \"document\":docs_txt}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzTp6vhdx37u"
      },
      "source": [
        "### question Rewriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "pR0nBm9dxibS"
      },
      "outputs": [],
      "source": [
        "### Question Re-writer\n",
        "# Prompt\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "\n",
        "\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEaB3Z6FyP6n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "OSSR93SayIYX",
        "outputId": "81234295-bc26-42f2-fa43-127f7920de7a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Here are a few options for improved questions, depending on what aspect of \"Agent Seoul\" you\\'re most interested in:\\n\\n*   **\"Who is Agent Seoul?\"** (This is good if you want to know the identity of the person/character/entity known as Agent Seoul)\\n*   **\"Agent Seoul: Background and Information\"** (This is a more general query, suitable if you want a broad overview)\\n*   **\"Agent Seoul: What is the real name?\"** (This is a good question if you are trying to find out the real name of the person who is known as \"Agent Seoul\"\\n*   **\"Agent Seoul: Career details\"** (This is a good question if you are trying to find out the career details of the person who is known as \"Agent Seoul\"\\n*   **\"Agent Seoul: Is Agent Seoul a real person?\"** (This is a good question if you are trying to find out if Agent Seoul is a real person)'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_rewriter.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "2sAkZYkWyRe8"
      },
      "outputs": [],
      "source": [
        "### Search\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptvDS8Fqyt1Y"
      },
      "outputs": [],
      "source": [
        "web_search_tool({\"query\":\"tell me about Taj Mahal\"})[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWoQ5UEWy1mC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPJ7Ku6UHrPF"
      },
      "source": [
        "### langgrap 실습 및 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdVuuDhifEJb"
      },
      "source": [
        "### 웹페이지 로드 및 벡터 DB 구축함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "pdA-BsOhhZ4U"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\"\n",
        ")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\"\n",
        ")\n",
        "llm_groq = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0,\n",
        "    max_retries=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbilHNsHi0p9",
        "outputId": "b96c6167-0851-4d41-ff56-1f8936babf24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ],
      "source": [
        "!pip  install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "kyk-IlHnfLhR"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "\n",
        "def setup_vectordb(urs:List):\n",
        "  docs = [WebBaseLoader(url).load() for url in urs]\n",
        "  docs_list =[item for sublist in docs for item in sublist]\n",
        "  # docs_list\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "      chunk_size=250, chunk_overlap=10\n",
        "  )\n",
        "\n",
        "  doc_splits = text_splitter.split_documents(docs_list)\n",
        "  vector_store = FAISS.from_documents(doc_splits , embedding=embeddings)\n",
        "  vector_store.save_local(\"/content/drive/MyDrive/DB/faiss_index\")\n",
        "  vectorstore = Chroma.from_documents(\n",
        "      documents=doc_splits,\n",
        "      embedding=embeddings,\n",
        "      collection_name=\"langgraph\",\n",
        "      persist_directory=\"/content/drive/MyDrive/DB/chroma_db\"\n",
        "  )\n",
        "\n",
        "  retriever = vectorstore.as_retriever()\n",
        "  return retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "mVWnqWMagtx1"
      },
      "outputs": [],
      "source": [
        "# RAG 체인 설정\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 문서 관련성 평가를 위한 구조체\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "# 문서 평가 설정\n",
        "def setup_grader(llm):\n",
        "    structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "    system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "\n",
        "    grade_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ])\n",
        "\n",
        "    return grade_prompt | structured_llm_grader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "lJ6MjWb3awBF"
      },
      "outputs": [],
      "source": [
        "# 질문 재작성 설정\n",
        "def setup_question_rewriter(llm):\n",
        "    system = \"\"\"You are a question re-writer that converts an input question to a better version that is optimized\n",
        "    for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\n",
        "\n",
        "    Your task is to improve the question to maximize the chance of finding relevant information. Focus on:\n",
        "    1. Clarifying ambiguous terms\n",
        "    2. Adding specific keywords related to the topic\n",
        "    3. Phrasing it in a way that would match informational content\n",
        "\n",
        "    Keep the question concise and focused on the main topic.\n",
        "    \"\"\"\n",
        "\n",
        "    re_write_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question that will get the best search results.\",\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "    return re_write_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO0P7TrXlbWa"
      },
      "source": [
        "### langgraph 아키텍쳐 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qDkiUUHeZ78_"
      },
      "outputs": [],
      "source": [
        "# node 정의\n",
        "def retrieve(state:GraphState):\n",
        "  print(\"---RETRIEVE NODE\")\n",
        "  return GraphState(question=question, documents=documents)\n",
        "\n",
        "def grade_documents(state:GraphState):\n",
        "  print(\"---CHECKING DOCUMENT RELEVANT IS TO QUESTION OR NOT---\")\n",
        "  web_search =\"yes\"\n",
        "  filtered_docs= []\n",
        "  return GraphState(question=question,web_search=web_search,documents=filtered_docs,)\n",
        "\n",
        "def generate(state:GraphState):\n",
        "  print(\"---GENERATE---\")\n",
        "  return GraphState(question=\"question\",orginal_question=\"orginal_question\", documents=[\"Hello\"], generation=[\"AAA\"])\n",
        "\n",
        "def transform_query(state:GraphState):\n",
        "  print(\"---TRANSFORM QUERY---\")\n",
        "  return GraphState(question=\"better question\",orginal_question=\"orginal_question\", documents=[\"web Hello\"])\n",
        "\n",
        "def web_search(state:GraphState):\n",
        "  print(\"---WEB SEARCH---\")\n",
        "  return GraphState(question=\"better question\",orginal_question=\"orginal_question\", documents=[\"web Hello\"])\n",
        "\n",
        "\n",
        "# node 추가\n",
        "from langgraph.graph import END, StateGraph ,START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"retrieve_node\", retrieve)\n",
        "workflow.add_node(\"grade_documents_node\", grade_documents)\n",
        "workflow.add_node(\"generate_node\", generate)\n",
        "workflow.add_node(\"transform_query_node\", transform_query)\n",
        "workflow.add_node(\"web_search_node\", web_search)\n",
        "\n",
        "#node를 edge로 연결\n",
        "workflow.add_edge(START , \"retrieve_node\")\n",
        "workflow.add_edge(\"retrieve_node\" , \"grade_documents_node\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents_node\", # 시작 노드 이름\n",
        "    decide_to_generate, # 조건 함수\n",
        "    {\n",
        "        \"웹검색\":\"transform_query_node\",\n",
        "        \"결과조합\":\"generate_node\" # 결과 키:노드 매핑\n",
        "    }\n",
        ")\n",
        "workflow.add_edge(\"transform_query_node\" , \"web_search_node\")\n",
        "workflow.add_edge(\"web_search_node\" , \"generate_node\")\n",
        "\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Szlv-H3IFRi"
      },
      "source": [
        "#### state 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rGfxGFsHux9",
        "outputId": "d7154450-6d0a-4f96-fe67-d3d6f88e108f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'documents': ['fewpfjwepfjwpejf', 'fewfewfwefwef']}"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from typing import List , Dict , Any ,TypedDict ,Annotated\n",
        "from langchain.schema import Document\n",
        "\n",
        "#전달해주는 state 상태정의\n",
        "class State(TypedDict):\n",
        "  question:str\n",
        "  orginal_question:str\n",
        "  documents:List[Document]\n",
        "  web_search:str\n",
        "  generation:str\n",
        "  web_results:List[Dict[str,Any]]\n",
        "  relevance_score:str\n",
        "\n",
        "class GraphState(State):\n",
        "  question:Annotated[str , \"user qeustion\" ]\n",
        "  documents:Annotated[List[Document] , []]\n",
        "  orginal_question:Annotated[str , \"original question\" ]\n",
        "  web_search:Annotated[str , \"web search\" ]\n",
        "  generation:Annotated[str , \"generation\" ]\n",
        "  web_results:Annotated[List[Dict[str,Any]] , [{}]]\n",
        "  relevance_score:Annotated[str , \"relevance_score\" ]\n",
        "\n",
        "GraphState(documents=[\"fewpfjwepfjwpejf\" , \"fewfewfwefwef\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwjwMkavKvgS"
      },
      "source": [
        "### Node 정의\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "UV4QZ1rjIQJs"
      },
      "outputs": [],
      "source": [
        "#retrieve 정의함수\n",
        "def retrieve(state:GraphState):\n",
        "  print(\"---Retrieve---\")\n",
        "  question = state[\"question\"]\n",
        "\n",
        "  #검색 실행\n",
        "  documents = retriever.get_relevant_documents(question)\n",
        "  # return State(documents=[f\"jfwpjfpewpfwejpfjpwef{question}\"])\n",
        "  return GraphState(question=question, documents=documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWWUlQAyMFFy",
        "outputId": "65556e48-4736-4084-d4d3-4ba0a8074702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---Retrieve---\n"
          ]
        }
      ],
      "source": [
        "documents = retrieve(State(question=\"tell me about SEOUL\"))\n",
        "# documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "RvKFKHGxMHrw"
      },
      "outputs": [],
      "source": [
        "#grade 노드\n",
        "def grade_documents(state:GraphState):\n",
        "  \"\"\"\n",
        "  check document relevance to question\n",
        "  \"\"\"\n",
        "  print(\"---CHECKING DOCUMENT RELEVANT IS TO QUESTION OR NOT---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  #문서가 없으면 바로 웹 검색하도록\n",
        "  if not documents:\n",
        "    return GraphState(\n",
        "        question=question,\n",
        "        orginal_question=state.get(\"original_questrion\", question),\n",
        "        documents=[],\n",
        "        web_search=\"yes\",\n",
        "        web_results=[{}],\n",
        "        relevance_score=\"no\")\n",
        "\n",
        "  # 문서 평가\n",
        "  filtered_docs = []\n",
        "  web_search =\"no\"\n",
        "  relevant_count = 0\n",
        "  for document in documents:\n",
        "    score= retriever_grader.invoke(\n",
        "        {\n",
        "            \"question\":question,\n",
        "            \"document\":document.page_content\n",
        "        }\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "    if grade == \"yes\":\n",
        "      print(\"---GRADE : DOCUMENT RELEVANT---\")\n",
        "      filtered_docs.append(document)\n",
        "      relevant_count +=1\n",
        "    else:\n",
        "      print(\"---GRADE : DOCUMENT NOT RELEVANT---\")\n",
        "\n",
        "  #관련성 너무 적으면 웹서치 할 수 있도록 함\n",
        "  if relevant_count < 2:\n",
        "    web_search = \"yes\"\n",
        "    print(f\"---ONLY {relevant_count} RELEVANT DOCUMENTS, WEB SEARCH NEEDED---\")\n",
        "  # return GraphState(\n",
        "  #     question=question,\n",
        "  #     orginal_question=state.get(\"original_questrion\", question),\n",
        "  #     documents=filtered_docs,\n",
        "  #     web_search=web_search,\n",
        "  #     web_results=[{}],\n",
        "  #     relevance_score=\"yes\" if filtered_docs else \"no\"\n",
        "  # )\n",
        "\n",
        "  return GraphState(question=question,web_search=web_search,documents=filtered_docs,)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "OzI8U1C_TUFd"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "def transform_query(state:GraphState):\n",
        "  \"\"\"질문 재작성\"\"\"\n",
        "  print(\"---TRANSFORM QUERY---\")\n",
        "  system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "\n",
        "\n",
        "  re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\", system),\n",
        "          (\n",
        "              \"human\",\n",
        "              \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "          ),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "  #처음 질문 저장\n",
        "  original_question = state.get(\"original_question\", state[\"question\"])\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "  #질문 재작성\n",
        "  better_question = question_rewriter.invoke({\"question\":question})\n",
        "  return GraphState(\n",
        "      question=better_question ,\n",
        "      orginal_question=original_question,\n",
        "      documents=documents\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "47cipRb6qC5g"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "web_search_tool = TavilySearchResults(k=5)\n",
        "def web_search(state:GraphState):\n",
        "  print(\"---WEB SEARCH---\")\n",
        "  question = state[\"question\"]\n",
        "  original_question = state.get(\"original_question\", question)\n",
        "  print(f\"---SEARCHING WEB FOR :{question}---\")\n",
        "  search_results = web_search_tool.invoke({\"query\":question})\n",
        "\n",
        "  #검색 결과 처리\n",
        "  if not search_results:\n",
        "    print(\"---NO WEB RESULTS FOUND---\")\n",
        "    return GraphState(\n",
        "        question=question,\n",
        "        orginal_question=original_question,\n",
        "        documents=[],\n",
        "        web_search=\"Complete\",\n",
        "        web_results=[],\n",
        "        relevance_score=\"no\"\n",
        "    )\n",
        "  web_docs = []\n",
        "  for result in search_results:\n",
        "    content = result.get(\"content\" , \"\")\n",
        "    title = result.get(\"title\",\"\")\n",
        "    if content:\n",
        "      metadata = {\"source\" : result.get(\"url\",\"\") , \"title\":title}\n",
        "      web_docs.append(Document(page_content=content , metadata=metadata))\n",
        "  print(f\"---FOUND {len(web_docs)} WEB RESULTS---\")\n",
        "  return GraphState(question=question ,orginal_question=original_question, documents=state[\"documents\"], web_search=\"Complete\",web_results=search_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "w6DOFJNvTfJF"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "def generate(state):\n",
        "    \"\"\"답변 생성\"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "\n",
        "    question = state[\"original_question\"] if \"original_question\" in state else state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG 프롬프트 준비\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "    # 검색 결과가 없는 경우 처리\n",
        "    if not documents:\n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"original_question\": state.get(\"original_question\", question),\n",
        "            \"documents\": [],\n",
        "            \"web_search\": state.get(\"web_search\", \"No\"),\n",
        "            \"web_results\": state.get(\"web_results\", []),\n",
        "            \"generation\": \"I couldn't find any relevant information to answer your question.\"\n",
        "        }\n",
        "\n",
        "    # RAG 체인으로 답변 생성\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "\n",
        "    # 웹 검색 결과 사용 여부 표시\n",
        "    if state.get(\"web_search\") == \"Completed\" and state.get(\"web_results\"):\n",
        "        sources = []\n",
        "        for idx, result in enumerate(state.get(\"web_results\", [])[:3], 1):\n",
        "            if \"url\" in result:\n",
        "                sources.append(f\"{idx}. {result.get('title', 'Source ' + str(idx))}: {result['url']}\")\n",
        "\n",
        "        if sources:\n",
        "            generation += \"\\n\\nSources:\\n\" + \"\\n\".join(sources)\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"original_question\": state.get(\"original_question\", question),\n",
        "        \"documents\": documents,\n",
        "        \"web_search\": state.get(\"web_search\", \"No\"),\n",
        "        \"web_results\": state.get(\"web_results\", []),\n",
        "        \"generation\": generation\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "Q1WJzF8qvpbQ"
      },
      "outputs": [],
      "source": [
        "def decide_to_generate(state:GraphState):\n",
        "  print(\"---ACCESS GRADES DOCUMENS---\")\n",
        "\n",
        "  web_search = state.get(\"web_search\", \"no\")\n",
        "\n",
        "  if web_search == \"yes\":\n",
        "    print(\"---DOCUMENT NOT RELEVANT! WEB SEARCH NEEDED---\")\n",
        "    return \"웹검색\"\n",
        "  elif web_search == \"Complete\":\n",
        "    print(\"--- DECISION : WEB SEARCH COMPLETE , GENERATE---\")\n",
        "    return \"결과조합\"\n",
        "  else:\n",
        "    print(\"--- DECSION! GENERATE---\")\n",
        "    return \"결과조합\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "lxLvX2zib0vV"
      },
      "outputs": [],
      "source": [
        "# 설정 및 실행 함수\n",
        "def setup_and_run(user_question, example_urls=None):\n",
        "    global retriever, retrieval_grader, question_rewriter, web_search_tool\n",
        "\n",
        "    # 벡터 DB 설정\n",
        "    if example_urls:\n",
        "        retriever = setup_vectordb(example_urls)\n",
        "    else:\n",
        "        # 기본 URL\n",
        "        default_urls = [\n",
        "            \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "            \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "            \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "        ]\n",
        "        retriever = setup_vectordb(default_urls)\n",
        "\n",
        "    # 그레이더 설정\n",
        "    retrieval_grader = setup_grader(llm)\n",
        "\n",
        "    # 질문 재작성기 설정\n",
        "    question_rewriter = setup_question_rewriter(llm)\n",
        "\n",
        "    # 웹 검색 도구 설정\n",
        "    web_search_tool = TavilySearchResults(k=5)\n",
        "\n",
        "    # 그래프 구성\n",
        "    workflow = StateGraph(State)\n",
        "\n",
        "    # 노드 추가\n",
        "    workflow.add_node(\"retrieve\", retrieve)\n",
        "    workflow.add_node(\"grade_documents\", grade_documents)\n",
        "    workflow.add_node(\"generate\", generate)\n",
        "    workflow.add_node(\"transform_query\", transform_query)\n",
        "    workflow.add_node(\"web_search_node\", web_search)\n",
        "\n",
        "    # 엣지 추가\n",
        "    workflow.add_edge(START, \"retrieve\")\n",
        "    workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "    workflow.add_conditional_edges(\n",
        "        \"grade_documents\",\n",
        "        decide_to_generate,\n",
        "        {\n",
        "            \"웹검색\": \"transform_query\",\n",
        "            \"결과조합\": \"generate\",\n",
        "        }\n",
        "    )\n",
        "    workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
        "    workflow.add_edge(\"web_search_node\", \"generate\")\n",
        "    workflow.add_edge(\"generate\", END)\n",
        "\n",
        "    # 그래프 컴파일\n",
        "    app = workflow.compile()\n",
        "\n",
        "    # 실행\n",
        "    inputs = {\"question\": user_question}\n",
        "    result = app.invoke(inputs)\n",
        "\n",
        "    # 결과 반환\n",
        "    return result[\"generation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "rv1PkPybcbNY",
        "outputId": "a8665f44-7f5b-4044-b035-714bab41889d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---Retrieve---\n",
            "---CHECKING DOCUMENT RELEVANT IS TO QUESTION OR NOT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---ONLY 0 RELEVANT DOCUMENTS, WEB SEARCH NEEDED---\n",
            "---ACCESS GRADES DOCUMENS---\n",
            "---DOCUMENT NOT RELEVANT! WEB SEARCH NEEDED---\n",
            "---TRANSFORM QUERY---\n",
            "---WEB SEARCH---\n",
            "---SEARCHING WEB FOR :Here are a few options for improved search queries, depending on the specific intent:\n",
            "\n",
            "*   **General Overview:** \"Seoul city guide\" or \"Seoul travel information\"\n",
            "*   **History:** \"History of Seoul South Korea\"\n",
            "*   **Culture:** \"Culture in Seoul\" or \"Traditions of Seoul\"\n",
            "*   **Tourism:** \"Top attractions in Seoul\" or \"Things to do in Seoul\"\n",
            "*   **Current Events:** \"News from Seoul South Korea\"\n",
            "\n",
            "I'd recommend using \"Seoul travel guide\" as this is a great starting point.---\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'get'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-4b8eb872856b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msetup_and_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tell me about SEOUL\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-100-1dccf220365b>\u001b[0m in \u001b[0;36msetup_and_run\u001b[0;34m(user_question, example_urls)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# 결과 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2892\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2894\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   2895\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2528\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-4f8076df16ec>\u001b[0m in \u001b[0;36mweb_search\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mweb_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
          ]
        }
      ],
      "source": [
        "setup_and_run(\"tell me about SEOUL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrySAVo1ZGNn",
        "outputId": "d032efc7-5a84-4ffb-fa45-5eb1b1a08188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---TRANSFORM QUERY---\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Here are a few options for rewritten questions, depending on the specific intent behind the original question:\\n\\n**Option 1 (General Information):**\\n\\n*   **Search Query:** \"Seoul South Korea information\"\\n\\n    *Reasoning:* This is a broad search aimed at getting general facts and details about the city.\\n\\n**Option 2 (History):**\\n\\n*   **Search Query:** \"History of Seoul\"\\n\\n    *Reasoning:*  If the user is interested in the historical background of Seoul.\\n\\n**Option 3 (Tourism):**\\n\\n*   **Search Query:** \"Tourist attractions in Seoul\"\\n\\n    *Reasoning:* If the user is planning a trip and wants to know what to see.\\n\\n**Option 4 (Culture):**\\n\\n*   **Search Query:** \"Culture and traditions in Seoul\"\\n\\n    *Reasoning:* If the user is interested in the cultural aspects of the city.\\n\\n**Option 5 (Modern Seoul):**\\n\\n*   **Search Query:** \"Modern Seoul architecture and lifestyle\"\\n\\n    *Reasoning:* If the user is interested in contemporary aspects of the city.\\n\\n**Choice Justification:**\\n\\nWithout more context, Option 1 is the most likely best choice as it covers general information. However, the other options are more focused and would provide better results if the user has a specific area of interest.',\n",
              " 'orginal_question': 'tell me about SEOUL',\n",
              " 'documents': [Document(metadata={'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'language': 'en', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.'}, page_content=\"With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]}"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform_query(grade_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SX5C5bj42gxL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed0M-XsuUqP5",
        "outputId": "7070e0c5-aad1-4181-ef65-15ce1c64aa59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "prompt_1 = \"\"\"\n",
        "      You are an assistant for question-answering tasks.\n",
        "      Use the following pieces of retrieved context to answer the question.\n",
        "      If you don't know the answer,\n",
        "      just say that i am a stupid.\n",
        "      Use five sentences maximum and keep the answer concise.\\n\n",
        "\n",
        "      Question: {question} \\n\n",
        "\n",
        "\n",
        "      Context: {context} \\nAnswer:\n",
        "\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZ_KRWqSzSZK"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image , display\n",
        "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q7stoV6o92w"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb4MXvTEo86u"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "tsK5Cm8LYvIr"
      },
      "outputs": [],
      "source": [
        "### Question Re-writer\n",
        "# Prompt\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "\n",
        "\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "O2gL2RqM7D0O"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "web_search_tool = TavilySearchResults(k=5)\n",
        "state = web_search_tool.invoke({\"query\":\"서울에 대해서\"})[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIluGVut76F3",
        "outputId": "517abd85-66c5-4d1b-8dda-e7f26d7c9810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---TRANSFORM QUERY---\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Here are a few options for rephrasing the question \"tell me about Busan\" for optimal web search, depending on the specific information desired:\\n\\n**Option 1 (General Overview):**\\n\\n*   **Best:** \"Busan South Korea travel guide\"\\n\\n**Reasoning:** This targets travel-related information, which is a common reason for people to search for information about a city.\\n\\n**Option 2 (Broader Information):**\\n\\n*   **Good:** \"Busan South Korea information\"\\n\\n**Reasoning:** This is a broad but direct query that should return a wide range of results about Busan, including its history, culture, and economy.\\n\\n**Option 3 (If you suspect the user is unfamiliar with Busan):**\\n\\n*   **Good:** \"Where is Busan? What is Busan known for?\"\\n\\n**Reasoning:** This explicitly asks for location and key characteristics.\\n\\nThe best option depends on what the user is *most likely* trying to find out. Without more context, I would recommend \"Busan South Korea travel guide\" as the most likely to be useful.',\n",
              " 'orginal_question': 'tell me about SEOUL',\n",
              " 'documents': ['Hello']}"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform_query({\"question\":\"tell me about Busan\",\"original_question\":\"tell me about SEOUL\",\"documents\":[\"Hello\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeFD5lyZS6Fs",
        "outputId": "c8aa16bf-9cf0-4185-abee-a9ce28a34843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---CHECKING DOCUMENT RELEVANT IS TO QUESTION OR NOT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---GRADE : DOCUMENT RELEVANT---\n",
            "---GRADE : DOCUMENT NOT RELEVANT---\n",
            "---ONLY 1 RELEVANT DOCUMENTS, WEB SEARCH NEEDED---\n"
          ]
        }
      ],
      "source": [
        "grade_state = grade_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXCnET797GrW"
      },
      "outputs": [],
      "source": [
        "web_search({})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
